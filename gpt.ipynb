{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05373fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31c3076c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c2edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(data)\n",
    "stoi, itos  = {}, {}\n",
    "for idx, val in enumerate(unique):\n",
    "    itos[idx] = val\n",
    "    stoi[val] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "11ecd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi) #65\n",
    "context = 64\n",
    "n_embd = 128\n",
    "n_layers = 4\n",
    "max_iters = 20000\n",
    "eval_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8bea8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, context_len):\n",
    "    X,Y = [],[]\n",
    "    x,y = [],[]\n",
    "    for idx, val in enumerate(data):\n",
    "        if idx % context == 0 and idx!=0:\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            x = []\n",
    "            y = []\n",
    "        x.append(stoi[val])\n",
    "        if idx+1 != len(data):\n",
    "            y.append(stoi[data[idx+1]])\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "261927ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_loss():\n",
    "    model.eval()\n",
    "    losses = {'train_loss':0.0, 'val_loss':0.0}\n",
    "    for loss_type,(X,Y) in [('train_loss', (Xtr, Ytr)), ('val_loss', (Xval, Yval))]:\n",
    "        for i in range(eval_iters):\n",
    "            ix = torch.randint(0, len(X)-batch_size, (1,))\n",
    "            x,targets = X[ix: ix+batch_size], Y[ix: ix+batch_size]\n",
    "            x = x.to(device)\n",
    "            targets = targets.to(device)\n",
    "            y = model(x)\n",
    "            y = y.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            loss = loss_fn(y, targets) \n",
    "            losses[loss_type]+=loss.item()\n",
    "    model.train()\n",
    "    return losses['train_loss']/eval_iters, losses['val_loss']/eval_iters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "089680df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13942, 13942, 3485, 3485)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training and validation split = 0.8, 0.2\n",
    "tr_len = int(0.8 * len(data))\n",
    "Xtr,Ytr = build_dataset(data[:tr_len], context)\n",
    "Xval,Yval = build_dataset(data[tr_len:], context)\n",
    "len(Xtr), len(Ytr), len(Xval), len(Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ceb4dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "afc8388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Blocks = nn.Sequential(*[Block() for _ in range(n_layers)])\n",
    "        self.Linear = nn.Linear(n_embd, vocab_size)\n",
    "        self.char_embd = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embd = nn.Embedding(context, n_embd)\n",
    "    def forward(self, x):\n",
    "        B,T = x.shape\n",
    "        x = self.char_embd(x) + self.pos_embd(torch.arange(T, device=device))\n",
    "        x = self.Blocks(x)\n",
    "        x = self.Linear(x)\n",
    "        return x\n",
    "        \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.K = nn.Linear(n_embd, head_size)\n",
    "        self.V = nn.Linear(n_embd, head_size)\n",
    "        self.Q = nn.Linear(n_embd, head_size)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        q = self.Q(x)\n",
    "        mask = torch.ones(T, T, device=device).tril()\n",
    "        qk = (q @ k.transpose(-2, -1)) / (n_embd ** 0.5)\n",
    "        qk = qk.masked_fill(mask==0, float('-inf'))\n",
    "        qk = F.softmax(qk, dim=-1)\n",
    "        out = qk @ v\n",
    "        return out\n",
    "    \n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_heads\n",
    "        self.Linear = nn.Linear(n_embd, n_embd)\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.Linear(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L1 = nn.Linear(n_embd, 4*n_embd)\n",
    "        self.L2 = nn.Linear(4*n_embd, n_embd)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.L1(x))\n",
    "        x = self.L2(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mh = MultiHead()\n",
    "        self.ff = FeedForward()\n",
    "        self.LN1 = LayerNorm(dim=n_embd)\n",
    "        self.LN2 = LayerNorm(dim=n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mh(self.LN1(x))\n",
    "        x = x + self.ff(self.LN2(x))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim, requires_grad=True))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim, requires_grad=True))        \n",
    "    def forward(self, x):\n",
    "        xmean = x.mean(-1, keepdims=True)\n",
    "        xvar = x.var(-1, keepdims=True)\n",
    "        xhat = (x - xmean) /torch.sqrt(xvar + self.eps)\n",
    "        x = self.gamma * xhat + self.beta\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2960ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.param_groups[0]['lr'] = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b13a99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 817985\n"
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "batch_size=32\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model=Transformer()\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "print(\"Total parameters\", sum(param.numel() for param in model.parameters() if param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c0f05a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0) train_loss: 4.443907737731934\n",
      "step 1000) train_loss: 2.081941024184227\n",
      "step 2000) train_loss: 1.6153456494808196\n",
      "step 3000) train_loss: 1.4800929639339446\n",
      "step 4000) train_loss: 1.3980403349399566\n",
      "step 5000) train_loss: 1.339493542432785\n",
      "step 6000) train_loss: 1.3004326119422913\n",
      "step 7000) train_loss: 1.2561045010089875\n",
      "step 8000) train_loss: 1.2193564192652702\n",
      "step 9000) train_loss: 1.1891900815963745\n",
      "step 10000) train_loss: 1.160729613661766\n",
      "step 11000) train_loss: 1.1317611409425736\n",
      "step 12000) train_loss: 1.1020720192790032\n",
      "step 13000) train_loss: 1.0745562453866004\n",
      "step 14000) train_loss: 1.0468373215794564\n",
      "step 15000) train_loss: 1.0227905620932578\n",
      "step 16000) train_loss: 1.0028964006304741\n",
      "step 17000) train_loss: 0.9807252017855644\n",
      "step 18000) train_loss: 0.9677421643137932\n",
      "step 19000) train_loss: 0.9451982621252537\n",
      "step 19999) train_loss: 0.9327040359377861\n"
     ]
    }
   ],
   "source": [
    "running_mean = 0.0\n",
    "for i in range(0, max_iters):\n",
    "    ix = torch.randint(0, len(Xtr)-batch_size, (1,))\n",
    "    x,targets = Xtr[ix: ix+batch_size], Ytr[ix: ix+batch_size]\n",
    "    x = x.to(device)\n",
    "    targets = targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y = model(x)\n",
    "    y = y.view(-1, vocab_size)\n",
    "    targets = targets.view(-1)\n",
    "    loss = loss_fn(y, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_mean += loss.item()\n",
    "    if i%1000==0 or i==max_iters-1:\n",
    "        print(f'step {i}) train_loss: {running_mean/1000 if i!=0 else loss.item()}')\n",
    "        running_mean=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.zeros(len(Xval)-batch_size)\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(Xval) - batch_size):\n",
    "        x,targets = Xval[ix: ix+batch_size], Yval[ix: ix+batch_size]\n",
    "        x = x.to(device)\n",
    "        targets = targets.to(device)\n",
    "        y = model(x)\n",
    "        y = y.view(-1, vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "        loss = loss_fn(y, targets)\n",
    "        losses[idx]=loss.item()\n",
    "        print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fe47b33e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8736/4239191741.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlast_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: zeros(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prompt = torch.zeros(1, context, dtype=torch.long, device=device) + 9\n",
    "    last_idx = 0\n",
    "    for _ in range(2000):\n",
    "        y=model(prompt)\n",
    "        y = F.softmax(y, dim=-1)\n",
    "        probs =y[0][last_idx]\n",
    "        pred = torch.multinomial(probs, 1)\n",
    "        if last_idx == context-1:\n",
    "            prompt = torch.hstack((prompt, torch.tensor([[0]], device=device)))\n",
    "        prompt[0][last_idx+1] = pred\n",
    "        print(itos[prompt[0][last_idx].item()], end=\"\")\n",
    "        if last_idx == context-1:\n",
    "            prompt = torch.unsqueeze(prompt[0][1:], dim=0)\n",
    "        else:\n",
    "            last_idx+=1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "PATH = './saved_model'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e600472b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (Blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (mh): MultiHead(\n",
       "        (Linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (L1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (L2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (LN1): LayerNorm()\n",
       "      (LN2): LayerNorm()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (mh): MultiHead(\n",
       "        (Linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (L1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (L2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (LN1): LayerNorm()\n",
       "      (LN2): LayerNorm()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (mh): MultiHead(\n",
       "        (Linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (L1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (L2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (LN1): LayerNorm()\n",
       "      (LN2): LayerNorm()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (mh): MultiHead(\n",
       "        (Linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (K): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (Q): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (L1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (L2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (LN1): LayerNorm()\n",
       "      (LN2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (Linear): Linear(in_features=128, out_features=65, bias=True)\n",
       "  (char_embd): Embedding(65, 128)\n",
       "  (pos_embd): Embedding(64, 128)\n",
       ")"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "173bdae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CINIUS:\n",
      "Bant thou shalt know which I am they doth love!\n",
      "Speak thy shape, what?\n",
      "\n",
      "POLIXENES:\n",
      "A partent, Edward, but, my true kins' lady!\n",
      "\n",
      "CORIOLANUS:\n",
      "Look with!\n",
      "Let the part of mine! 'What easch ever quitied.\n",
      "A fall of his destruction\n",
      "Before, a jain out of o'eries, his dislord,\n",
      "Which is and buting what doless thou lack,\n",
      "And yet on the lisance of winted thy lord.\n",
      "\n",
      "LADY:\n",
      "Why lord, when we chappinius, we will exclord;\n",
      "Then set noble reafning to the Lord Horsh,\n",
      "Than king another, or sir! well take gue oft!\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Welcome often at eagle's my love with a carflage:\n",
      "He sea, that let's heart him I can either presung\n",
      "And let us with Clorm o' the child day,\n",
      "I would you have not monealeng for than you:\n",
      "He foe, him God, and po to-night!\n",
      "Think you heaven like affect with you well?\n",
      "\n",
      "ERCHARD:\n",
      "All-boor living finel, chook and you beheapt of him:\n",
      "Andful Duke of London; your lovy took;\n",
      "thing only revoice, I should you but see\n",
      "WARWICK: and admers am Bestruck and cast thou:\n",
      "You part as thy chambest thou disin of him:\n",
      "'Tis gone hath wint a Jupod's hope,\n",
      "And not more than ere for I\n",
      "Pleave them done, but see that it woe grace.\n",
      "And take fall to be Edward's there's quoth Faulh,\n",
      "Here't, whom I should shalt cabry the firth\n",
      "Of God presently the pear.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "CLORD IONT:\n",
      "Alabited, he,ilever way.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "KING EDWARD IV:\n",
      "Where's monefests of me; but I have for yourse,\n",
      "In thehe grial in grace's haste, a Tyrror,\n",
      "And I am the right partly corn't.\n",
      "\n",
      "LEONTES:\n",
      "I'll not gone away as Countolinus,\n",
      "The trimorth o' Tirrief me thee from fair quainta.\n",
      "Now, proviler, why then will heart about seve:\n",
      "And thou, but more I am sirrew fair;\n",
      "And plothen the sun of my rest!\n",
      "Prove will tender proused charity, queen,\n",
      "And not for this more at the duke fath't:\n",
      "Uncle my pomply, have offenced to these,\n",
      "And savouring we forborness or theme,\n",
      "Where rure heart his than all honours;\n",
      "And came intend on The Juliet.\n",
      "\n",
      "CAMILLO:\n",
      "In doubt meet there more favour-aming York.\n",
      "\n",
      "FLORIZEL:\n",
      "Here it you--stethink, Pither, I t"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long, device=device) + 18\n",
    "with torch.no_grad():\n",
    "    for _ in range(2000):\n",
    "        prompt = idx[:, -context:]\n",
    "        y = model(prompt)\n",
    "        probs = F.softmax(y, dim=-1)\n",
    "        pred = probs[:, -1, :]\n",
    "        next_idx = torch.multinomial(pred, 1)\n",
    "        print(itos[idx[0][-1].item()], end=\"\")\n",
    "        idx = torch.cat((idx, next_idx), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1de2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
